{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421311e6",
   "metadata": {},
   "source": [
    "Q1] What is Logistic Regression, and how does it differ from Linear Regression?  \n",
    "Ans] Logistic Regression is a supervised machine learning algorithm used for classification problems.  \n",
    "It predicts the probability that a given input belongs to a certain class (category).  \n",
    "Letâ€™s say you want to predict whether a student will pass (1) or fail (0) based on study hours.  \n",
    "Input: X = study hours  \n",
    "Output: Y = 1 (pass) or 0 (fail)  \n",
    "Instead of predicting a continuous number (like Linear Regression does), Logistic Regression predicts:  \n",
    "The probability that Y = 1 (student passes).  \n",
    "\n",
    "Since probabilities must be between 0 and 1, Logistic Regression uses a sigmoid function to â€œsquashâ€ the output of a linear equation.    \n",
    "1 Linear Regression is used to predict a continuous numeric value, while Logistic Regression is used to predict a categorical outcome such as Yes/No or 0/1.  \n",
    "\n",
    "2 In Linear Regression, the output can take any real value (from negative infinity to positive infinity), whereas in Logistic Regression, the output is always between 0 and 1, representing a probability.  \n",
    "\n",
    "3 Linear Regression fits a straight line to the data, showing a linear relationship between the input and output, while Logistic Regression fits an S-shaped (sigmoid) curve to map values into probabilities.  \n",
    "\n",
    "4 Linear Regression directly predicts a numeric value, but Logistic Regression predicts a probability, which is then converted into a class label using a threshold (commonly 0.5).  \n",
    "\n",
    "5 Linear Regression produces a straight-line decision boundary, whereas Logistic Regression produces a sigmoid-shaped probability boundary that helps separate classes.  \n",
    "\n",
    "6 Linear Regression assumes a linear relationship between the independent and dependent variables, whereas Logistic Regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable.  \n",
    "\n",
    "7 The error function (or loss function) used in Linear Regression is Mean Squared Error (MSE), while Logistic Regression uses Log Loss (Binary Cross-Entropy).  \n",
    "\n",
    "8 Linear Regression is applied to problems like predicting house prices, sales, or temperature, while Logistic Regression is used for predicting spam detection, disease diagnosis, or customer churn.  \n",
    "\n",
    "10 Finally, Linear Regression is meant for regression tasks (continuous outputs), and Logistic Regression is meant for classification tasks (discrete outputs).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876082e",
   "metadata": {},
   "source": [
    "2] Explain the role of the Sigmoid function in Logistic Regression?  \n",
    "Ans] The Sigmoid function is a mathematical function that converts any real number (from âˆ’âˆž to +âˆž) into a value between 0 and 1.  \n",
    "In Logistic Regression, we want to predict the probability that a data point belongs to a certain class (e.g., 1 = yes, 0 = no).\n",
    "But the linear equation   \n",
    "b0â€‹+b1â€‹x can produce any value â€” positive, negative, or zero.  \n",
    "Thatâ€™s not suitable for a probability (which must be between 0 and 1).  \n",
    "\n",
    "So, the Sigmoid function â€œsquashesâ€ this linear output into the range [0, 1].  \n",
    "1ï¸âƒ£ Compute linear combination  \n",
    "Logistic Regression first computes:  z=b0â€‹+b1â€‹x1â€‹+b2â€‹x2â€‹+â‹¯+bnâ€‹xnâ€‹  \n",
    "\n",
    "2ï¸âƒ£ Apply the Sigmoid function  \n",
    "Then it applies the sigmoid transformation\n",
    "\n",
    "3ï¸âƒ£ Interpret the result  \n",
    "if p>0.5: predict class 1  \n",
    "If pâ‰¤0.5: predict class 0  \n",
    "\n",
    "4ï¸âƒ£ Use in model training  \n",
    "The model adjusts coefficients b0â€‹,b1â€‹,â€¦ to minimize the difference between predicted probabilities and actual class labels (using log loss).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900473d9",
   "metadata": {},
   "source": [
    "3] What is Regularization in Logistic Regression and why is it needed?  \n",
    "Ans] Regularization is a technique used to prevent overfitting in a model by adding a penalty term to the modelâ€™s loss (cost) function.  \n",
    "When training Logistic Regression (or any model), the algorithm tries to minimize a loss function â€” in this case, the log loss (or cross-entropy loss).\n",
    "\n",
    "If we donâ€™t control the model, it might:  \n",
    "Fit too closely to the training data (overfitting)  \n",
    "Learn noise or irrelevant patterns  \n",
    "Perform poorly on new (test) data  \n",
    "To fix this, regularization adds an extra term that penalizes large weights (coefficients) â€” forcing the model to stay simpler and more generalizable.  \n",
    "Types of Regularization  \n",
    "L1 Regularization (Lasso)  \n",
    "L2 Regularization (Ridge)  \n",
    "Elastic Net  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae2294",
   "metadata": {},
   "source": [
    "4] What are some common evaluation metrics for classification models, and why are they important?  \n",
    "Ans] Evaluation metrics are quantitative measures used to assess how well a classification model performs â€” that is, how accurately and reliably it predicts classes (like 0/1 or Yes/No).  \n",
    "\n",
    "1] Accuracy  \n",
    "The ratio of correctly predicted observations to the total observations.  \n",
    "Accuracy=TP+TN+FP+FN/TP+TN  \n",
    "TP = True Positives (correctly predicted 1s)  \n",
    "TN = True Negatives (correctly predicted 0s)  \n",
    "FP = False Positives (wrongly predicted 1s)  \n",
    "FN = False Negatives (missed 1s)  \n",
    "\n",
    "Precision  \n",
    "Out of all the positive predictions, how many were actually correct?  \n",
    "Precision=ð‘‡ð‘ƒ/ð‘‡ð‘ƒ+ð¹ð‘ƒ\tâ€‹  \n",
    "Measures how reliable positive predictions are.  \n",
    "Important when false positives are costly (e.g., spam detection â€” you donâ€™t want to mark genuine emails as spam).   \n",
    "\n",
    "Recall (Sensitivity or True Positive Rate)  \n",
    "Out of all actual positives, how many did the model correctly predict?  \n",
    "Recall=TP/TP+FN  \n",
    "Measures how comprehensive the model is in finding positives.  \n",
    "Important when missing a positive case is costly (e.g., disease detection â€” you donâ€™t want to miss an actual patient).  \n",
    "\n",
    "F1-Score  \n",
    "The harmonic mean of Precision and Recall.    \n",
    "F1Â Score=2Ã—  PrecisionÃ—Recall/Precision+Recall  \n",
    "Balances both Precision and Recall.  \n",
    "Useful when you have imbalanced classes.  \n",
    "A single metric to evaluate trade-off between FP and FN.  \n",
    "\n",
    "ROC Curve (Receiver Operating Characteristic) & AUC (Area Under Curve)  \n",
    "Plots True Positive Rate (Recall) vs False Positive Rate (1 - Specificity) for different thresholds.  \n",
    "\n",
    "Confusion Matrix  \n",
    "Gives a complete picture of model performance.  \n",
    "Helps calculate all other metrics like Precision, Recall, and F1-score.  \n",
    "\tâ€‹\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9f63b",
   "metadata": {},
   "source": [
    "5] : Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
    "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
    "(Use Dataset from sklearn package)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8194040a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy: 1.00\n",
      "[[12  0]\n",
      " [ 0  8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "\n",
    "df = df[df['target'] != 2]\n",
    "\n",
    "X = df[iris.feature_names]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941306f",
   "metadata": {},
   "source": [
    "6] Write a Python program to train a Logistic Regression model using L2\n",
    "regularization (Ridge) and print the model coefficients and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6070de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients (weights for each feature):\n",
      "sepal length (cm): -0.3754\n",
      "sepal width (cm): -1.3966\n",
      "petal length (cm): 2.1525\n",
      "petal width (cm): 0.9642\n",
      "Intercept: -0.2564\n",
      "\n",
      "Logistic Regression Model Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "df = df[df['target'] != 2]\n",
    "\n",
    "X = df[iris.feature_names]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model coefficients (weights for each feature):\")\n",
    "for feature, coef in zip(iris.feature_names, model.coef_[0]):\n",
    "    print(f\"{feature}: {coef:.4f}\")\n",
    "\n",
    "print(f\"Intercept: {model.intercept_[0]:.4f}\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nLogistic Regression Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd28b2",
   "metadata": {},
   "source": [
    "7] Write a Python program to train a Logistic Regression model for multiclass\n",
    "classification using multi_class='ovr' and print the classification report.\n",
    "(Use Dataset from sklearn package)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be1b5817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Multiclass Logistic Regression (OvR):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\COMP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "X = df[iris.feature_names]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report for Multiclass Logistic Regression (OvR):\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb6d45",
   "metadata": {},
   "source": [
    "8] Write a Python program to apply GridSearchCV to tune C and penalty\n",
    "hyperparameters for Logistic Regression and print the best parameters and validation\n",
    "accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc0916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 10, 'penalty': 'l1'}\n",
      "Best Cross-Validation Accuracy: 0.96\n",
      "Test Set Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "X = df[iris.feature_names]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=500, solver='liblinear')  \n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],          \n",
    "    'penalty': ['l1', 'l2']                \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                           cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317957d5",
   "metadata": {},
   "source": [
    "9]Write a Python program to standardize the features before training Logistic\n",
    "Regression and compare the model's accuracy with and without scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "763da640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy WITHOUT scaling: 0.96\n",
      "Accuracy WITH scaling: 0.97\n",
      "\n",
      "Comparison:\n",
      "Without scaling: 0.96\n",
      "With scaling   : 0.97\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "X = df[data.feature_names]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model_no_scaling = LogisticRegression(max_iter=5000, solver='liblinear')\n",
    "model_no_scaling.fit(X_train, y_train)\n",
    "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
    "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
    "print(f\"Accuracy WITHOUT scaling: {accuracy_no_scaling:.2f}\")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "model_scaled = LogisticRegression(max_iter=5000, solver='liblinear')\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(f\"Accuracy WITH scaling: {accuracy_scaled:.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Without scaling: {accuracy_no_scaling:.2f}\")\n",
    "print(f\"With scaling   : {accuracy_scaled:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eaa33b",
   "metadata": {},
   "source": [
    "10] Imagine you are working at an e-commerce company that wants to\n",
    "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
    "dataset (only 5% of customers respond), describe the approach youâ€™d take to build a\n",
    "Logistic Regression model â€” including data handling, feature scaling, balancing\n",
    "classes, hyperparameter tuning, and evaluating the model for this real-world business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2346de92",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, roc_auc_score\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[32m     11\u001b[39m X, y = make_classification(\n\u001b[32m     12\u001b[39m     n_samples=\u001b[32m5000\u001b[39m,        \n\u001b[32m     13\u001b[39m     n_features=\u001b[32m10\u001b[39m,       \n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m df = pd.DataFrame(X, columns=[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mfeature_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X.shape[\u001b[32m1\u001b[39m])])\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,        \n",
    "    n_features=10,       \n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.95, 0.05],  \n",
    "    flip_y=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "df['target'] = y\n",
    "\n",
    "print(\"Original class distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_scaled, y)\n",
    "\n",
    "print(\"\\nClass distribution AFTER SMOTE:\")\n",
    "print(pd.Series(y_res).value_counts())\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear'] \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',   \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "print(grid.best_params_)\n",
    "\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_prob = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5001f868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.14.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.25.2 in c:\\users\\comp\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn->imblearn) (2.2.4)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in c:\\users\\comp\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn->imblearn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in c:\\users\\comp\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in c:\\users\\comp\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\comp\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn->imblearn) (3.6.0)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading imbalanced_learn-0.14.0-py3-none-any.whl (239 kB)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "\n",
      "   ---------------------------------------- 0/2 [imbalanced-learn]\n",
      "   ---------------------------------------- 0/2 [imbalanced-learn]\n",
      "   ---------------------------------------- 0/2 [imbalanced-learn]\n",
      "   ---------------------------------------- 2/2 [imblearn]\n",
      "\n",
      "Successfully installed imbalanced-learn-0.14.0 imblearn-0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
