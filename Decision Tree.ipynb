{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f690f4",
   "metadata": {},
   "source": [
    "Q1] What is a Decision Tree, and how does it work in the context of\n",
    "classification?\n",
    "Ans] A Decision Tree is a supervised machine learning algorithm used for both classification and regression, but it is most commonly used for classification tasks. It works by splitting the dataset into branches based on feature values, eventually leading to a decision (a class label).  \n",
    "\n",
    "Start at the Root Node  \n",
    "The algorithm considers the entire dataset and tries to find the feature that best separates the classes (e.g., \"Age\", \"Income\", etc.).  \n",
    "Select the Best Feature to Split  \n",
    "Uses a measure such as:  \n",
    "Gini Impurity  \n",
    "Entropy / Information Gain  \n",
    "\n",
    "Gain Ratio  \n",
    "These measures help determine how well a feature divides the data into pure groups (groups where most samples belong to the same class).  \n",
    "\n",
    "Split the Data Based on the Feature  \n",
    "If the condition is:  \n",
    "Age > 30?  \n",
    "The dataset splits into two groups: Age > 30 and Age ≤ 30.  \n",
    "Repeat the Process  \n",
    "Each subgroup is then split again based on another feature, forming new branches.  \n",
    "\n",
    "Stop Splitting When:  \n",
    "All values belong to one class (pure node), or  \n",
    "Maximum tree depth is reached, or  \n",
    "Not enough samples to split further.  \n",
    "Output the Class Label at the Leaf Node  \n",
    "When traversal reaches a leaf, the leaf stores the most common class in that subset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3cec57",
   "metadata": {},
   "source": [
    "Q2] Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
    "How do they impact the splits in a Decision Tree?  \n",
    "Ans]In a decision tree, before splitting the data at each node, the algorithm must decide which feature and threshold will best separate the classes.  \n",
    "To do this, it measures how “impure” or “mixed” a node is.  \n",
    "A node is pure if it contains only one class.  \n",
    "A node is impure if it contains a mixture of classes.  \n",
    "Two common impurity measures are:  \n",
    "Gini Impurity  \n",
    "Entropy (Information Gain)  \n",
    "\n",
    "Gini Impurity:  \n",
    "Gini measures the probability of incorrect classification if we randomly assign a label based on class proportions.  \n",
    "Lower Gini → purer node.  \n",
    "\n",
    "Entropy:  \n",
    "Entropy measures uncertainty or disorder in the data.  \n",
    "Entropy is 0 when the node is pure.  \n",
    "Entropy is maximum when classes are equally mixed.  \n",
    "\n",
    "How They Impact the Splits (Points)  \n",
    "\n",
    "Used to evaluate split quality:  \n",
    "At each decision point, the tree calculates either Gini Impurity or Entropy to determine how pure or mixed a node is.  \n",
    "\n",
    "Lower impurity = better split:  \n",
    "The algorithm always prefers splits that result in child nodes with lower impurity (more pure classes).  \n",
    "\n",
    "Gini focuses on misclassification reduction:  \n",
    "Gini Impurity tries to reduce the chance of incorrectly classifying a randomly chosen sample.  \n",
    "\n",
    "Entropy focuses on information gain:  \n",
    "Entropy selects splits that maximize information gain, meaning it aims to reduce uncertainty in the data.  \n",
    "\n",
    "Both try to separate classes effectively:  \n",
    "The chosen split should group similar class samples together, increasing purity in nodes.  \n",
    "\n",
    "Entropy tends to produce more balanced splits:  \n",
    "It considers all class proportions carefully, sometimes leading to more evenly distributed branches.  \n",
    "\n",
    "Gini tends to isolate the dominant class faster:  \n",
    "It is slightly more sensitive to majority class frequency, so it may create simpler and faster splits.  \n",
    "\n",
    "The algorithm tests multiple split points:  \n",
    "For each feature, the tree tries possible thresholds and calculates impurity values.  \n",
    "\n",
    "The best split minimizes weighted impurity:  \n",
    "The final chosen split is the one with the lowest weighted impurity across child nodes.  \n",
    "\n",
    "Impurity measures continue at each level:  \n",
    "This process is repeated recursively until stopping conditions are met (like max depth or pure nodes).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166e4d4",
   "metadata": {},
   "source": [
    "Q3] What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
    "Trees? Give one practical advantage of using each.  \n",
    "Ans] Pre-Pruning (Early Stopping)  \n",
    "Pruning is done during the tree-building process.  \n",
    "The tree stops splitting when further splits do not significantly improve performance.  \n",
    "Controlled using parameters like max_depth, min_samples_split, min_samples_leaf, etc.  \n",
    "Helps avoid overfitting early by limiting tree growth.  \n",
    "Faster and more efficient because the tree is smaller from the start.  \n",
    "Risk: It may stop too early, causing underfitting.  \n",
    "Practical Advantage:  \n",
    "✅ Saves time and computational resources by preventing unnecessary growth.  \n",
    "\n",
    "Post-Pruning (Cost Complexity Pruning)  \n",
    "Pruning is done after the tree has been fully grown.  \n",
    "The tree is first allowed to overfit, then unnecessary branches are removed.  \n",
    "Uses evaluation metrics (e.g., cross-validation) to prune weak splits.  \n",
    "Helps reduce overfitting and improves generalization.  \n",
    "Typically more accurate but takes longer to compute.  \n",
    "Requires additional validation data to decide which branches to remove.  \n",
    "\n",
    "Practical Advantage:  \n",
    "✅ Produces a simpler model with better accuracy on unseen data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365263c5",
   "metadata": {},
   "source": [
    "Q4] What is Information Gain in Decision Trees, and why is it important for\n",
    "choosing the best split?  \n",
    "Ans] Information Gain is a measure used in Decision Trees to determine which feature provides the best split at each node.  \n",
    "It tells us how much uncertainty (impurity) is reduced after splitting the data using a particular feature.  \n",
    "It is calculated using Entropy.  \n",
    "Information Gain=Entropy (Parent Node)−Weighted Entropy (Child Nodes)  \n",
    "So, higher Information Gain means the split makes the classes more separated (purer).   \n",
    "Why Is Information Gain Important for Choosing the Best Split?  \n",
    "\n",
    "Helps Identify the Most Useful Feature:  \n",
    "It selects the feature that best separates the data based on class labels.  \n",
    "\n",
    "Reduces Uncertainty:  \n",
    "A split with high Information Gain reduces disorder (entropy) and creates purer child nodes.  \n",
    "\n",
    "Improves Model Accuracy:  \n",
    "By choosing splits that maximize Information Gain, the tree learns better decision boundaries.  \n",
    "\n",
    "Prevents Random or Unhelpful Splits:  \n",
    "Without Information Gain, the tree might split on features that do not help classification.  \n",
    "\n",
    "Builds the Tree Efficiently:  \n",
    "Highest Information Gain = most informative split → faster and more effective tree building.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1e219",
   "metadata": {},
   "source": [
    "Q5] What are some common real-world applications of Decision Trees, and\n",
    "what are their main advantages and limitations?  \n",
    "Ans] Real-World Applications of Decision Trees  \n",
    "\n",
    "Medical Diagnosis  \n",
    "Used to predict diseases based on symptoms, test results, age, etc.  \n",
    "Example: Classifying patients as “High Risk” or “Low Risk.”  \n",
    "\n",
    "Credit Risk Assessment (Banking & Finance)  \n",
    "Used to decide whether to approve or deny loan applications.  \n",
    "Example: Predicting if a customer will default based on income, credit score, etc.  \n",
    "\n",
    "Customer Churn Prediction (Marketing)  \n",
    "Helps identify customers likely to leave a service.  \n",
    "Used for targeted retention campaigns.  \n",
    "\n",
    "Fraud Detection  \n",
    "Detects unusual transaction patterns that may indicate fraud.  \n",
    "\n",
    "Recommendation Systems  \n",
    "Identifies customer preferences for product/service recommendations.  \n",
    "\n",
    "Manufacturing Quality Control  \n",
    "Classifies products as “Defective” or “Non-defective” based on features.  \n",
    "\n",
    "Human Resource Decision Making  \n",
    "Used in employee performance evaluation and promotion eligibility.   \n",
    "\n",
    "Advantages of Decision Trees  \n",
    "\n",
    "Easy to Understand and Interpret  \n",
    "Decision trees resemble human thinking and can be visualized.  \n",
    "\n",
    "No Need for Feature Scaling  \n",
    "Works well with both categorical and numerical data without normalization.  \n",
    "\n",
    "Handles Nonlinear Relationships  \n",
    "Can model complex decision boundaries.  \n",
    "\n",
    "Can Handle Missing Values  \n",
    "Some implementations allow splitting based on available data only.  \n",
    "\n",
    "Fast Prediction  \n",
    "Once built, the tree makes decisions quickly.  \n",
    "\n",
    "Limitations of Decision Trees  \n",
    "\n",
    "High Risk of Overfitting  \n",
    "Trees may grow too large and fit training data too closely.  \n",
    "\n",
    "Unstable  \n",
    "Small changes in data can lead to a completely different tree.  \n",
    "\n",
    "Biased Toward Features with Many Levels  \n",
    "Categorical features with many categories can dominate splits.  \n",
    "\n",
    "Not Always the Most Accurate Alone  \n",
    "Often improved using Ensembles like Random Forest or Gradient Boosting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25a81f",
   "metadata": {},
   "source": [
    "Q6] Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Train a Decision Tree Classifier using the Gini criterion\n",
    "● Print the model’s accuracy and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8371751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n",
      "\n",
      "Feature Importances:\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0167\n",
      "petal length (cm): 0.9061\n",
      "petal width (cm): 0.0772\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data        \n",
    "y = iris.target     \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
    "    print(f\"{name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b9489",
   "metadata": {},
   "source": [
    "Q7] Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
    "a fully-grown tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360dc28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Fully-Grown Tree:  1.0\n",
      "Accuracy of Tree with max_depth=3:  1.0\n",
      "\n",
      "Both models performed equally.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data          # Features\n",
    "y = iris.target        # Labels\n",
    "\n",
    "# 2. Split into training and test sets (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train a fully-grown Decision Tree\n",
    "full_tree = DecisionTreeClassifier(random_state=42)\n",
    "full_tree.fit(X_train, y_train)\n",
    "full_pred = full_tree.predict(X_test)\n",
    "full_accuracy = accuracy_score(y_test, full_pred)\n",
    "\n",
    "# 4. Train a Decision Tree with max_depth = 3\n",
    "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "pruned_tree.fit(X_train, y_train)\n",
    "pruned_pred = pruned_tree.predict(X_test)\n",
    "pruned_accuracy = accuracy_score(y_test, pruned_pred)\n",
    "\n",
    "# 5. Print the accuracies\n",
    "print(\"Accuracy of Fully-Grown Tree: \", full_accuracy)\n",
    "print(\"Accuracy of Tree with max_depth=3: \", pruned_accuracy)\n",
    "\n",
    "# Optional: Show which model performed better\n",
    "if pruned_accuracy > full_accuracy:\n",
    "    print(\"\\nPruned tree generalizes better (less overfitting).\")\n",
    "elif pruned_accuracy < full_accuracy:\n",
    "    print(\"\\nFully-grown tree performed better, but may overfit.\")\n",
    "else:\n",
    "    print(\"\\nBoth models performed equally.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1b2bb",
   "metadata": {},
   "source": [
    "Q8] Write a Python program to:\n",
    "● Load the Boston Housing Dataset\n",
    "● Train a Decision Tree Regressor\n",
    "● Print the Mean Squared Error (MSE) and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beab45cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.495235205629094\n",
      "\n",
      "Feature Importances:\n",
      "MedInc: 0.5285\n",
      "HouseAge: 0.0519\n",
      "AveRooms: 0.0530\n",
      "AveBedrms: 0.0287\n",
      "Population: 0.0305\n",
      "AveOccup: 0.1308\n",
      "Latitude: 0.0937\n",
      "Longitude: 0.0829\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Load the California Housing Dataset (replacement for Boston Housing)\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data               # Features\n",
    "y = housing.target             # Target (median house value)\n",
    "\n",
    "# 2. Split into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train the Decision Tree Regressor\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 5. Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "# 6. Print Feature Importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(housing.feature_names, model.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac6210",
   "metadata": {},
   "source": [
    "Q9] Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
    "GridSearchCV\n",
    "● Print the best parameters and the resulting model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e30d871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
      "Model Accuracy with Best Parameters: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Iris Dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2. Split into training and testing data (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Define a Decision Tree Classifier\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# 4. Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 3, 4, 5, 10]\n",
    "}\n",
    "\n",
    "# 5. Perform Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 6. Print the best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# 7. Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model Accuracy with Best Parameters:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad612a4d",
   "metadata": {},
   "source": [
    "Q10] Imagine you’re working as a data scientist for a healthcare company that\n",
    "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
    "mixed data types and some missing values.\n",
    "Explain the step-by-step process you would follow to:\n",
    "● Handle the missing values\n",
    "● Encode the categorical features\n",
    "● Train a Decision Tree model\n",
    "● Tune its hyperparameters\n",
    "● Evaluate its performance\n",
    "And describe what business value this model could provide in the real-world\n",
    "setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc83dd5",
   "metadata": {},
   "source": [
    "A. Handle missing values\n",
    "Audit missingness: compute % missing per feature and check patterns (MCAR / MAR / MNAR).\n",
    "Keep missingness flags: for important features add feature_missing = isnull indicator — missingness can be predictive.\n",
    "Simple imputation baseline:\n",
    "Numeric: median (robust to outliers) or mean if symmetric.\n",
    "Categorical: most frequent or \"MISSING\" label.\n",
    "Advanced imputation (if needed):\n",
    "IterativeImputer (model-based) or KNN imputer when relationships exist.\n",
    "Use caution: imputation leakage — fit imputer only on training fold.\n",
    "Document assumptions: why you imputed and what values mean clinically.\n",
    "\n",
    "B. Encode categorical features\n",
    "Categorize by cardinality:\n",
    "Low cardinality (<= ~10 unique): One-Hot Encoding.\n",
    "High cardinality: Target encoding / leave-one-out / frequency encoding to avoid huge sparse matrices. Decision Trees tolerate ordinal numeric encoding — but beware of introducing spurious order.\n",
    "Fit encoders only on training data (use ColumnTransformer / Pipeline to avoid leakage).\n",
    "Preserve rare categories: map rare levels to __OTHER__.\n",
    "\n",
    "C. Train the Decision Tree model\n",
    "Start with a pipeline: preprocessing (imputer, encoder, scaler if needed for other models) → DecisionTreeClassifier.\n",
    "Class imbalance: if positive cases are rare, use class_weight='balanced' or resampling (SMOTE) inside cross-validation.\n",
    "Baseline: train default tree, record metrics.\n",
    "\n",
    "D. Tune hyperparameters\n",
    "Use GridSearchCV or RandomizedSearchCV with StratifiedKFold.\n",
    "Tune: max_depth, min_samples_split, min_samples_leaf, criterion (gini/entropy), max_features, class_weight.\n",
    "Consider cost-sensitive tuning: use custom scoring that weights false negatives higher if missing disease is costly.\n",
    "Use nested CV if you need an unbiased estimate of generalization performance.\n",
    "\n",
    "E. Evaluate performance\n",
    "Primary metrics (healthcare context):\n",
    "Recall / Sensitivity (catch sick patients)\n",
    "Precision (avoid too many false alarms)\n",
    "ROC-AUC and PR-AUC (PR especially if class imbalance)\n",
    "Confusion matrix at operational threshold; consider threshold tuning using ROC/PR or business cost function.\n",
    "Calibration: check predicted probabilities (reliability diagrams, calibration curve); calibrate with CalibratedClassifierCV if needed.\n",
    "Explainability: feature importances, SHAP values, decision paths for sample patients.\n",
    "Robustness checks: performance across subgroups (age, gender, hospital), temporal validation (train on earlier period, test on later).\n",
    "Statistical significance / CI for metrics via bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38703232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "Best params: {'clf__class_weight': None, 'clf__criterion': 'gini', 'clf__max_depth': 3, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2}\n",
      "Accuracy: 1.0\n",
      "Recall (Sensitivity): 1.0\n",
      "Precision: 1.0\n",
      "ROC AUC: 1.0\n",
      "\n",
      "Confusion matrix:\n",
      " [[10  0]\n",
      " [ 0 20]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "\n",
      "Top feature importances:\n",
      " petal length (cm)    1.0\n",
      "sepal length (cm)    0.0\n",
      "sepal width (cm)     0.0\n",
      "petal width (cm)     0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Requires: scikit-learn >=0.24, category_encoders (optional)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (accuracy_score, recall_score, precision_score,\n",
    "                             roc_auc_score, confusion_matrix, classification_report)\n",
    "\n",
    "# --- Assume `df` is your DataFrame, 'target' is 0/1 where 1 = disease present\n",
    "# df = pd.read_csv('patient_data.csv')\n",
    "\n",
    "# Example placeholders (replace with your data)\n",
    "# df = ...\n",
    "# target_col = 'disease'\n",
    "# X = df.drop(columns=[target_col])\n",
    "# y = df[target_col]\n",
    "\n",
    "# For demonstration, using iris as surrogate (replace with your data)\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris(as_frame=True)\n",
    "df = iris.frame\n",
    "# Make it a binary problem by mapping classes 0 vs (1 or 2)\n",
    "df['target'] = (df['target'] != 0).astype(int)\n",
    "target_col = 'target'\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Identify column types (example logic)\n",
    "numeric_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    # Decision Trees don't need scaling, so we omit scaler\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Full pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('pre', preprocessor),\n",
    "    ('clf', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter grid for tuning\n",
    "param_grid = {\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    'clf__max_depth': [3, 5, 7, None],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4],\n",
    "    'clf__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=cv, scoring='recall', n_jobs=-1, verbose=1)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Evaluate on held-out test set (do a train_test_split first in real workflow)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:,1] if hasattr(best_model.named_steps['clf'], \"predict_proba\") else None\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Recall (Sensitivity):\", recall_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "if y_proba is not None:\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importances (map back to original feature names)\n",
    "# After preprocessing, need to reconstruct column names for OneHot encoder\n",
    "ohe_cols = []\n",
    "if categorical_features:\n",
    "    ohe = best_model.named_steps['pre'].named_transformers_['cat'].named_steps['onehot']\n",
    "    ohe_cols = list(ohe.get_feature_names_out(categorical_features))\n",
    "feature_names = numeric_features + ohe_cols\n",
    "importances = best_model.named_steps['clf'].feature_importances_\n",
    "feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "print(\"\\nTop feature importances:\\n\", feat_imp.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0262c0",
   "metadata": {},
   "source": [
    "3) Evaluation checklist (operational concerns)  \n",
    "Choose metric aligned with business cost (e.g., missing a diseased patient might cost more than false alarm).  \n",
    "Threshold selection: pick a probability threshold based on cost matrix, not necessarily 0.5.  \n",
    "Subgroup fairness: measure performance across demographics; mitigate biased performance.  \n",
    "Calibration: if probabilities are used for triage, ensure good calibration.  \n",
    "Temporal/External validation: test on later periods and different hospitals.  \n",
    "Explainability: provide per-prediction explanations (SHAP, simplest decision path).  \n",
    "Monitoring: set up drift detection (data distribution, performance drop).  \n",
    "\n",
    "4) Deployment, privacy & ethics notes  \n",
    "Data privacy: follow HIPAA/GDPR rules; de-identify PHI; control access to model outputs.  \n",
    "Human-in-the-loop: use model as triage/decision support, not as sole final decision maker.  \n",
    "Documentation: keep model card that lists intended use, limitations, and performance.  \n",
    "Regulatory: in some jurisdictions medical decision tools require approval; check compliance.  \n",
    "\n",
    "5) Business value (short & concrete)  \n",
    "Early detection → earlier interventions, better patient outcomes.  \n",
    "Efficient triage → prioritize high-risk patients for further tests or specialist review.  \n",
    "Cost reduction → avoid unnecessary tests for low-risk patients, allocate resources efficiently.  \n",
    "Operational planning → predict caseloads and resource demand (beds, staff).  \n",
    "Quality improvement → discover predictive features that inform new clinical guidelines.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
